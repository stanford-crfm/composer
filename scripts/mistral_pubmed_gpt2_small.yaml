model:
  gpt2:
    use_pretrained: false
    tokenizer_name: gpt2
    gradient_checkpointing: false
    model_config:
      activation_function: gelu_new
      architectures:
        - GPT2LMHeadModel
      attn_pdrop: 0.1
      bos_token_id: 50256
      embd_pdrop: 0.1
      eos_token_id: 50256
      initializer_range: 0.02
      layer_norm_epsilon: 1.0e-05
      model_type: gpt2
      n_ctx: 1024
      n_embd: 768
      n_head: 12
      # n_inner: 3072
      n_inner: null # mistral change
      n_layer: 12
      n_positions: 1024
      resid_pdrop: 0.1
      scale_attn_weights: true
      summary_activation: null
      summary_first_dropout: 0.1
      summary_proj_to_labels: true
      summary_type: cls_index
      summary_use_proj: true
      task_specific_params:
        text-generation:
          do_sample: true
          max_length: 50
      transformers_version: 4.18.0
      use_cache: true
      vocab_size: 50257
      # core mistral changes:
      reorder_and_upcast_attn: true
      scale_attn_by_inverse_layer_idx: true
