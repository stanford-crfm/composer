image: mosaicml/pytorch_mofed:jamie-temp
name: gpt2-125m-pubmed-openweb-nodes-4
git_repo: https://github.com/stanford-crfm/composer.git
git_branch: multinode_gpt2
instance: r6z1-g8-a100
num_nodes: 4

command: >-
  git clone {{ git_repo }}

  cd composer

  git checkout {{ git_branch }}

  pip install --user -e .[all]

  composer examples/run_composer_trainer.py -f /mnt/config/parameters.yaml --run_name {{ name }}

parameters:
  train_dataset:
    pubmed:
      name: pubmed_plus_openweb
      split: train
      num_samples: 330373010
      max_seq_len: 1024
      tokenizer_name: gpt2
      group_method: concat
      seed: 17
      shuffle: true
      shuffle_buffer_size: 100000
  val_dataset:
    pubmed:
      name: randomized
      split: validation
      num_samples: 63954
      max_seq_len: 1024
      tokenizer_name: gpt2
      group_method: concat
      seed: 17
      shuffle: false
      drop_last: false
  model:
    gpt2:
      use_pretrained: false
      tokenizer_name: gpt2
      model_config:
        activation_function: gelu_new
        architectures:
          - GPT2LMHeadModel
        attn_pdrop: 0.1
        bos_token_id: 50256
        embd_pdrop: 0.1
        eos_token_id: 50256
        initializer_range: 0.02
        layer_norm_epsilon: 1.0e-05
        model_type: gpt2
        n_ctx: 1024
        n_embd: 768
        n_head: 12
        # n_inner: 3072
        n_inner: null # mistral change
        n_layer: 12
        n_positions: 1024
        resid_pdrop: 0.1
        scale_attn_weights: true
        summary_activation: null
        summary_first_dropout: 0.1
        summary_proj_to_labels: true
        summary_type: cls_index
        summary_use_proj: true
        task_specific_params:
          text-generation:
            do_sample: true
            max_length: 50
        transformers_version: 4.16.2
        use_cache: true
        vocab_size: 50257
        # core mistral changes:
        reorder_and_upcast_attn: true
        scale_attn_by_inverse_layer_idx: true
  optimizer:
    decoupled_adamw:
      lr: 3.0e-4
      betas:
        - 0.9
        - 0.999
      eps: 1.0e-08
      weight_decay: 0.1
  schedulers:
    linear_decay_with_logarithmic_warmup:
      t_warmup: 0.01dur
  max_duration: 400000ba
  train_batch_size: 512
  grad_accum: 8 # TODO, is this right?
  eval_batch_size: 32
  seed: 17
  device:
    gpu: {}
  dataloader:
    pin_memory: true
    persistent_workers: true
    num_workers: 1
    timeout: 0
    prefetch_factor: 2
  precision: fp16
  deepspeed:
    zero_optimization:
      stage: 1
  grad_clip_norm: 1.0
  validate_every_n_batches: 1000
  validate_every_n_epochs: 1

  ### Logging ###
  callbacks:
    loss_scale_monitor: {}
    lr_monitor: {}
    speed_monitor:
      window_size: 5
  save_folder: "/tmp/{run_name}"
  save_artifact_name:  "{run_name}/checkpoints/rank{rank}"
  save_interval: 1000ba
  save_num_checkpoints_to_keep: 1
  loggers:
    progress_bar: {}
    wandb:
      project: mosaic-gpt2
      rank_zero_only: true
      log_artifacts: true
