train_dataset:
  c4:
    split: train
    num_samples: 51200000 # 1e11 tok ~= 51200000[sa] * 2048[msl] = 50000[ba] * 1024[bs] * 2048[msl]
    max_seq_len: 2048
    tokenizer_name: gpt2
    group_method: concat
    seed: 17
    shuffle: true
    drop_last: true
val_dataset:
  c4:
    split: validation
    num_samples: 364608 # approx total validation set
    max_seq_len: 2048
    tokenizer_name: gpt2
    group_method: truncate
    seed: 17
    shuffle: false
    drop_last: false
model:
  gpt2:
    use_pretrained: false
    tokenizer_name: gpt2
    gradient_checkpointing: true
    model_config:
      n_embd: 5120
      n_head: 40
      n_layer: 40
      n_positions: 2048
optimizer:
  decoupled_adamw:
    lr: 1.0e-4
    betas:
      - 0.9
      - 0.95
    eps: 1.0e-08
    weight_decay: 0.0
schedulers:
  - cosine_decay_with_warmup:
      t_warmup: 0.01dur
loggers:
  - tqdm: {}
max_duration: 1ep
train_batch_size: 1024 # 2e6 tok ~= 1024[bs] * 2048[msl]
grad_accum: 128 # 1024[bs] / 8[devices] / 1[micro_bs_per_device] = 128[ga], assuming 8xA100-80GB
eval_batch_size: 8 # 8[devices] * 1[micro_bs_per_device] = 8[bs], assuming 8xA100-80GB
seed: 17
device:
  gpu: {}
dataloader:
  pin_memory: true
  persistent_workers: true
  num_workers: 1
  timeout: 0
  prefetch_factor: 2
deepspeed:
  zero_optimization:
    stage: 2
precision: fp16
grad_clip_norm: 1.0
validate_every_n_batches: 2000
validate_every_n_epochs: 1
