image: davhall/codalab-composer:latest # #mosaicml/pytorch  # Name of the docker image to use
run_name: mistral_pubmed_gpt2_small_biomedical_tokenizer
#instance: r6z1-g8-a100
platform: r7z1
gpu_type: a100_40gb
gpu_num: 32
#num_nodes: 4

integrations:
  - integration_type: git_repo
    git_repo: stanford-crfm/composer
    git_branch: pubmed
    pip_install: '--user -e ".[all]"'
    ssh_clone: false

command: |
  set -e -x
  cd composer
  composer -n 8 examples/run_composer_trainer.py -f /mnt/config/parameters.yaml --run_name mistral_pubmed_gpt2_small_pubmed_tokenizer
parameters:
  train_dataset:
    sprucfluo:
        datasets:
          pubmed:
              urls:
                - "https://storage.googleapis.com/pubmed-mosaic/pubmed-sharded/pubmedAbs_train.{1..128}-of-128.jsonl.gz"
                - "https://storage.googleapis.com/pubmed-mosaic/pubmed-sharded/pubmedC_train.{1..128}-of-128.jsonl.gz"
        num_samples: 36708112000
        max_seq_len: 1024
        tokenizer_name: stanford-crfm/pubmed_gpt_tokenizer
        seed: 17
        cycle: true
        shuffle: true
        shuffle_buffer_size: 50000
#  val_dataset:
#    sprucfluo:
#        datasets:
#          pubmed:
#              urls:
#                # make it be 3
#                - "https://storage.googleapis.com/pubmed-mosaic/pubmed-sharded/pubmedRandomized_val.{1..8}-of-8.jsonl.gz"
#        max_seq_len: 1024
#        tokenizer_name: gpt2
#        num_samples: 20000
#        cycle: false
  model:
    gpt2:
      use_pretrained: false
      tokenizer_name: stanford-crfm/pubmed_gpt_tokenizer
      gradient_checkpointing: false
      model_config:
        activation_function: gelu_new
        architectures:
          - GPT2LMHeadModel
        attn_pdrop: 0.1
        bos_token_id: 28895
        embd_pdrop: 0.1
        eos_token_id: 28895
        initializer_range: 0.02
        layer_norm_epsilon: 1.0e-05
        model_type: gpt2
        n_ctx: 1024
        n_embd: 768
        n_head: 12
        # n_inner: 3072
        n_inner: null # mistral change
        n_layer: 12
        n_positions: 1024
        resid_pdrop: 0.1
        scale_attn_weights: true
        summary_activation: null
        summary_first_dropout: 0.1
        summary_proj_to_labels: true
        summary_type: cls_index
        summary_use_proj: true
        task_specific_params:
          text-generation:
            do_sample: true
            max_length: 50
        transformers_version: 4.18.0
        use_cache: true
        vocab_size: 28896
        # core mistral changes:
        reorder_and_upcast_attn: true
        scale_attn_by_inverse_layer_idx: true
  optimizer:
    adamw:
      lr: 6.0e-4
      betas:
        - 0.9
        - 0.999
      eps: 1.0e-08
      weight_decay: 0.1
  schedulers:
    linear_decay_with_warmup:
      t_warmup: 0.025dur
  max_duration: 400000ba
  train_batch_size: 512
  grad_accum: 16
  eval_batch_size: 128
  seed: 17
  device:
    gpu: {}
  dataloader:
    pin_memory: true
    persistent_workers: true
    num_workers: 1
    timeout: 0
    prefetch_factor: 2
  precision: fp16
  deepspeed:
    zero_optimization:
      stage: 1
  grad_clip_norm: 1.0
  validate_every_n_batches: 1000
  validate_every_n_epochs: 1

  ### Logging ###
  callbacks:
    lr_monitor: {}
    speed_monitor:
      window_size: 5
  save_folder: "/tmp/mistral_pubmed_gpt2_small_biomedical_tokenizer"
  save_artifact_name:  "mistral_pubmed_gpt2_small_biomedical_tokenizer/checkpoints/rank{rank}"
  save_interval: 20000ba
  save_num_checkpoints_to_keep: 1
  loggers:
    progress_bar: {}
    wandb:
      project: mosaic-gpt2
      name: "mistral_pubmed_gpt2_small_biomedical_tokenizer"
      rank_zero_only: true
      log_artifacts: true
