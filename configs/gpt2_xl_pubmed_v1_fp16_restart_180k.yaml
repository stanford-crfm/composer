image: davhall/codalab-composer:latest # #mosaicml/pytorch  # Name of the docker image to use
run_name: gpt2_xl_pubmed_v1_bf16_restart_180k
#instance: r6z1-g8-a100
platform: r6z1
gpu_type: a100_40gb
gpu_num: 32
#num_nodes: 4

integrations:
  - integration_type: git_repo
    git_repo: stanford-crfm/composer
    git_branch: pubmed
    pip_install: '--user -e ".[all]"'
    ssh_clone: false

command: |
  set -e -x
  cd composer
  composer examples/run_composer_trainer.py -f /mnt/config/parameters.yaml --run_name gpt2_xl_pubmed_v1_bf16_restart_180k
parameters:
  train_dataset:
    sprucfluo:
        datasets:
          pubmed:
              urls:
                - "https://storage.googleapis.com/pubmed-mosaic/pubmed-sharded/pubmedAbs_train.{1..128}-of-128.jsonl.gz"
                - "https://storage.googleapis.com/pubmed-mosaic/pubmed-sharded/pubmedC_train.{1..128}-of-128.jsonl.gz"
        num_samples: 36708112
        max_seq_len: 1024
        tokenizer_name: gpt2
        seed: 7
        cycle: true
        shuffle: true
        shuffle_buffer_size: 50000
#  val_dataset:
#    sprucfluo:
#        datasets:
#          pubmed:
#              urls:
#                - "https://storage.googleapis.com/pubmed-mosaic/pubmed-sharded/pubmedRandomized_val.{1..8}-of-8.jsonl.gz"
#        max_seq_len: 1024
#        tokenizer_name: gpt2
#        num_samples: 5000
#        shuffle: false
#        cycle: false
#        is_train: false
  model:
    gpt2:
      use_pretrained: true
      pretrained_model_name: stanford-crfm/pubmed_gpt
      pretrained_revision: pubmed_gpt_xl_180k
      tokenizer_name: gpt2
      gradient_checkpointing: true
      model_config:
        use_cache: false
        n_embd: 1600
        n_head: 25
        n_layer: 48
        n_positions: 1024
        reorder_and_upcast_attn: true
        scale_attn_by_inverse_layer_idx: true
  optimizer:
    decoupled_adamw:
      lr: 2.0e-4
      betas:
        - 0.9
        - 0.95
      eps: 1.0e-9
      weight_decay: 0.0
  schedulers:
    cosine_decay_with_warmup:
      t_warmup: 500ba
  max_duration: 340500ba # we made it 160k through, plus the (short) warmup
  train_batch_size: 512
  grad_accum: 16
  eval_batch_size: 128
  seed: 7
  device:
    gpu: {}
  dataloader:
    pin_memory: true
    persistent_workers: true
    num_workers: 1
    timeout: 0
    prefetch_factor: 2
  precision: fp16
  deepspeed:
    zero_optimization:
      stage: 1
  grad_clip_norm: 1.0
  validate_every_n_batches: 1000
  validate_every_n_epochs: 1

  ### Logging ###
  callbacks:
    lr_monitor: {}
    speed_monitor:
      window_size: 5
    loss_scale_monitor: {}
    grad_monitor:
      log_layer_grad_norms: True
  save_folder: "/tmp/gpt2_xl_pubmed_v1_bf16_restart_180k"
  save_artifact_name:  "gpt2_xl_pubmed_v1_bf16_restart_180k/checkpoints/rank{rank}"
  save_interval: 20000ba
  save_num_checkpoints_to_keep: 1
  loggers:
    progress_bar: {}
    wandb:
      project: mosaic-gpt2
      name: "gpt2_xl_pubmed_v1_bf16_restart_180k"
      rank_zero_only: true
      log_artifacts: true
